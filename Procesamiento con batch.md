## ğŸ§  Desarrollo del procesamiento batch

A continuaciÃ³n se describen los pasos realizados en el proyecto con los **comandos utilizados**.

---

### ğŸŸ¢ 1ï¸âƒ£ Crear la sesiÃ³n de Spark

Se inicializa una sesiÃ³n de Spark que permite ejecutar el procesamiento distribuido.  

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when

spark = SparkSession.builder \
    .appName("Tarea3_Procesamiento_Batch_COVID19") \
    .getOrCreate()

### ğŸŸ¢ 2ï¸âƒ£ Cargar el conjunto de datos desde HDFS


El dataset estÃ¡ almacenado en el sistema HDFS en la siguiente ruta:

hdfs://localhost:9000/Tarea3/Casos_positivos_de_COVID-19_en_Colombia._20251014.csv


CÃ³digo utilizado:

df = spark.read.option("header", True).option("inferSchema", True) \
    .csv("hdfs://localhost:9000/Tarea3/Casos_positivos_de_COVID-19_en_Colombia._20251014.csv")

print("\n=== Vista previa del dataset ===\n")
df.show(10)


Comando de terminal para ejecutar el script:

spark-submit tarea3_batch.py

### ğŸŸ¢ 3ï¸âƒ£ Limpieza y transformaciÃ³n de datos

Se eliminan los valores nulos y se transforman columnas para garantizar la consistencia del anÃ¡lisis.

CÃ³digo:

print("\n=== Limpieza y transformaciÃ³n de datos ===\n")

# Conteo de valores nulos
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

# Eliminar filas con valores nulos en campos clave
columnas_clave = ["Nombre departamento", "Nombre municipio", "Edad", "Sexo"]
df_clean = df.dropna(subset=columnas_clave)

# Convertir la edad a tipo entero
df_clean = df_clean.withColumn("Edad", col("Edad").cast("int"))

### ğŸŸ¢ 4ï¸âƒ£ AnÃ¡lisis exploratorio de datos (EDA)

Se realiza un anÃ¡lisis descriptivo utilizando operaciones con DataFrames:

print("\n=== AnÃ¡lisis exploratorio de datos (EDA) ===\n")

# EstadÃ­sticas descriptivas
print("ğŸ“Š EstadÃ­sticas descriptivas de la edad:")
df_clean.describe(["Edad"]).show()

# Conteo por sexo
print("ğŸ“ˆ Conteo de casos por sexo:")
df_clean.groupBy("Sexo").count().orderBy("Sexo").show()

# DistribuciÃ³n por estado de recuperaciÃ³n
print("ğŸ“ˆ DistribuciÃ³n de casos por estado de recuperaciÃ³n:")
df_clean.groupBy("Recuperado").count().orderBy("count", ascending=False).show()

# Casos por departamento
print("ğŸ“ Casos por departamento:")
df_clean.groupBy("Nombre departamento").count().orderBy("count", ascending=False).show(10)

### ğŸŸ¢ 5ï¸âƒ£ Almacenamiento de resultados

Los datos limpios y transformados se guardan nuevamente en HDFS, en formato CSV.

CÃ³digo:

print("\n=== Guardando resultados limpios en HDFS ===\n")
df_clean.write.mode("overwrite").option("header", True).csv("hdfs://localhost:9000/Tarea3/resultados")

print("\nâœ… Proceso completado con Ã©xito. Los resultados se guardaron en: hdfs://localhost:9000/Tarea3/resultados\n")

# Cierre de la sesiÃ³n
spark.stop()
