##  Desarrollo del procesamiento batch
En el procesamiento batch, se trabaj贸 con un conjunto de datos almacenado en HDFS, correspondiente a los casos positivos de COVID-19 en Colombia.
Se desarroll贸 una aplicaci贸n en PySpark que realiza las siguientes tareas:

- Carga del conjunto de datos desde el sistema distribuido HDFS.
- Limpieza y transformaci贸n de los datos (manejo de valores nulos, conversi贸n de tipos).
- An谩lisis exploratorio de datos (EDA) utilizando DataFrames.
- Generaci贸n de estad铆sticas descriptivas y conteos agrupados por sexo, edad y departamento.
- Almacenamiento de los resultados procesados nuevamente en HDFS.

**A continuaci贸n se describen los pasos realizados en el proyecto con los comandos utilizados.**

---

###  1锔 Crear la sesi贸n de Spark

Se inicializa una sesi贸n de Spark que permite ejecutar el procesamiento distribuido.  

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when

spark = SparkSession.builder \
    .appName("Tarea3_Procesamiento_Batch_COVID19") \
    .getOrCreate()
```


###  2锔 Cargar el conjunto de datos desde HDFS


El dataset est谩 almacenado en el sistema HDFS en la siguiente ruta:
```python
hdfs://localhost:9000/Tarea3/Casos_positivos_de_COVID-19_en_Colombia._20251014.csv
```

C贸digo utilizado:
```python
df = spark.read.option("header", True).option("inferSchema", True) \
    .csv("hdfs://localhost:9000/Tarea3/Casos_positivos_de_COVID-19_en_Colombia._20251014.csv")

print("\n=== Vista previa del dataset ===\n")
df.sho 
```
Comando para ejecutar el script:
```python
spark-submit tarea3.py
```
###  3锔 Limpieza y transformaci贸n de datos

Se eliminan los valores nulos y se transforman columnas para garantizar la consistencia del an谩lisis.

C贸digo:
```python
print("\n=== Limpieza y transformaci贸n de datos ===\n")

#Conteo de valores nulo
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

#Eliminar filas con valores nulos en campos clave
columnas_clave = ["Nombre departamento", "Nombre municipio", "Edad", "Sexo"]
df_clean = df.dropna(subset=columnas_clave)

#Convertir la edad a tipo entero
df_clean = df_clean.withColumn("Edad", col("Edad").cast("int"))
```
###  4锔 An谩lisis exploratorio de datos (EDA)

Se realiza un an谩lisis descriptivo utilizando operaciones con DataFrames:
```python
print("\n=== An谩lisis exploratorio de datos (EDA) ===\n")

# Estad铆sticas descriptivas
print("Estad铆sticas descriptivas de la edad:")
df_clean.describe(["Edad"]).show()

# Conteo por sexo
print(" Conteo de casos por sexo:")
df_clean.groupBy("Sexo").count().orderBy("Sexo").show()

# Distribuci贸n por estado de recuperaci贸n
print("Distribuci贸n de casos por estado de recuperaci贸n:")
df_clean.groupBy("Recuperado").count().orderBy("count", ascending=False).show()

# Casos por departamento
print("Casos por departamento:")
df_clean.groupBy("Nombre departamento").count().orderBy("count", ascending=False).show(10)
```
### 5锔 Almacenamiento de resultados

Los datos limpios y transformados se guardan nuevamente en HDFS, en formato CSV llamado resultados.

C贸digo:
```python
print("\n=== Guardando resultados limpios en HDFS ===\n")
df_clean.write.mode("overwrite").option("header", True).csv("hdfs://localhost:9000/Tarea3/resultados")

print("\n Proceso completado con 茅xito. Los resultados se guardaron en: hdfs://localhost:9000/Tarea3/resultados\n")

#Cierre de la sesi贸n
spark.stop()
```
